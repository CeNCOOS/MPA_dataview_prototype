{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import datetime as dt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "from urllib.request import Request, urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull HTML index file and make it into a list\n",
    "# (modified from https://stackoverflow.com/questions/11023530/python-to-list-http-files-and-directories)\n",
    "# filter list by .hdf files and extract those filenames\n",
    "# open each of these .hdf files with xarray via urlretrieve()\n",
    "\n",
    "def get_data_urls(url):\n",
    "    url = url.replace(\" \",\"%20\")\n",
    "    req = Request(url)\n",
    "    a = urlopen(req).read()\n",
    "    soup = BeautifulSoup(a, 'html.parser')\n",
    "    x = (soup.find_all('a'))\n",
    "    text_list = []\n",
    "    for i in x:\n",
    "        file_name = i.extract().get_text()\n",
    "        url_new = url + file_name\n",
    "        url_new = url_new.replace(\" \",\"%20\")\n",
    "        text_list.append(url_new)\n",
    "    datafile_list = list(filter(re.compile(\".*hdf\").match, text_list)) # Read Note\n",
    "  \n",
    "    return(datafile_list)\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to open a .hdf datafile from the Wimsoft site by running it through urlretrieve()\n",
    "\n",
    "def pull_hdf(url):\n",
    "    local_filename, headers = urllib.request.urlretrieve(url)\n",
    "    return(xr.open_dataset(local_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean up the .hdf datafile from the Wimsoft site \n",
    "\n",
    "def clean_hdf(dataset, year, doy):\n",
    "    var_name = str('sst_' + str(year) + doy)\n",
    "    \n",
    "    # add lat/long coordinates\n",
    "    dataset['fakeDim0'] = sst_coords['Latitude'].values[:,0]\n",
    "    dataset['fakeDim1'] = sst_coords['Longitude'].values[0,:]\n",
    "    \n",
    "    # rename lat, long, and sst variables\n",
    "    ds = dataset.rename({'fakeDim0': 'lat',\n",
    "                         'fakeDim1': 'lon',\n",
    "                         var_name: 'sst'})\n",
    "    \n",
    "    # create time and add it as a dimension to the hdf\n",
    "    date = dt.datetime(int(year), 1, 1) + dt.timedelta(int(doy) -1)\n",
    "    ds.coords['time'] = date\n",
    "    ds['sst'] = ds['sst'].assign_coords(time = date)\n",
    "    \n",
    "    # filter and convert sst values based on documentation\n",
    "    # data should be unsigned ints but are imported as signed\n",
    "    # to fix, negative values should have a constant 256 added to them\n",
    "    # then remove values of 0 and 255, which are invalid\n",
    "    ds = (ds.where(ds['sst'] > 0, ds['sst'] + 256)\n",
    "           .where((ds['sst'] != 0) & (ds['sst'] != 1) & (ds['sst'] != 255))\n",
    "         )\n",
    "    \n",
    "    # convert sst pixel values to degC\n",
    "    ds['sst'] = 0.15*ds['sst'] - 3.0\n",
    "\n",
    "    \n",
    "    return(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technically, should just be able to open this directly from Wimsoft but my connection is slow af\n",
    "# so I'm just going to load it from local disk instead  ¯\\_(ツ)_/¯\n",
    "\n",
    "# sst_coords = pull_hdf(\"http://wimsoft.com/CAL/files/cal_aco_3840_Latitude_Longitude.hdf\")\n",
    "\n",
    "sst_coords = xr.open_dataset(\"../../../Raw_Data/Wimsoft_SST/cal_aco_3840_Latitude_Longitude.hdf\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all the SST data, write them to netCDFs by year\n",
    "\n",
    "# year_list = [str(yr) for yr in range(2000, 2021)]\n",
    "\n",
    "year_list = [str(yr) for yr in range(2015, 2020)]\n",
    "\n",
    "for i, year in enumerate(year_list):\n",
    "    index_url = str('https://www.wimsoft.com/CAL/'+ year + '/M' + year + '_sst_day/')\n",
    "    url_list = get_data_urls(index_url)\n",
    "    print('found {} data files for {}'.format(len(url_list), year))\n",
    "    for j, url in enumerate(url_list): \n",
    "        doy = url.split('_')[2][-3:]   \n",
    "        hdf = pull_hdf(url)\n",
    "        ds = clean_hdf(hdf, year, doy)        \n",
    "        if j == 0:  \n",
    "            ds_all = ds\n",
    "        else:\n",
    "            ds_all = xr.concat([ds_all, ds], dim = 'time')\n",
    "        print('concatenating {}_{}'.format(year, doy))\n",
    "            \n",
    "    ds_all = ds_all.sortby('time')\n",
    "    print('writing year {} to netcdf'.format(year))\n",
    "    ds_all.to_netcdf('SST_winsoft_dataset/merged_sst_' + str(year) + '.nc', mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
